{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OpenAI provides state-of-the-art language models. You can use embeddings to compare text similarities.To highlight the reasoning improvement over GPT-4o, we tested our models on a diverse set of human exams', 'our models on a diverse set of human exams and ML benchmarks. We show that o1 significantly outperforms GPT-4o on the vast majority of these reasoning-heavy tasks. Unless otherwise specified,', 'vast majority of these reasoning-heavy tasks. Unless otherwise specified, we evaluated o1 on the maximal test-time compute setting.', 'In many reasoning-heavy benchmarks, o1 rivals the performance of human experts. Recent frontier models1 do so well on MATH2 and GSM8K that these benchmarks are no longer effective at differentiating', 'that these benchmarks are no longer effective at differentiating models. We evaluated math performance on AIME, an exam designed to challenge the brightest high school math students in America. On', 'the brightest high school math students in America. On the 2024 AIME exams, GPT-4o only solved on average 12% (1.8/15) of problems. o1 averaged 74% (11.1/15) with a single sample', 'problems. o1 averaged 74% (11.1/15) with a single sample per problem, 83% (12.5/15) with consensus among 64 samples, and 93% (13.9/15) when re-ranking 1000 samples with a learned scoring function.', 'when re-ranking 1000 samples with a learned scoring function. A score of 13.9 places it among the top 500 students nationally and above the cutoff for the USA Mathematical Olympiad.']\n",
      "Filtered chunks: ['that these benchmarks are no longer effective at differentiating models. We evaluated math performance on AIME, an exam designed to challenge the brightest high school math students in America. On', 'the brightest high school math students in America. On the 2024 AIME exams, GPT-4o only solved on average 12% (1.8/15) of problems. o1 averaged 74% (11.1/15) with a single sample', 'problems. o1 averaged 74% (11.1/15) with a single sample per problem, 83% (12.5/15) with consensus among 64 samples, and 93% (13.9/15) when re-ranking 1000 samples with a learned scoring function.']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import openai\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API')\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(api_key = openai_api_key)\n",
    "\n",
    "def recursive_text_splitter(text, chunk_size=30, overlap=9):\n",
    "    \"\"\"\n",
    "    Recursively splits text into chunks of a specified size with overlap.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to split.\n",
    "        chunk_size (int): The size of each chunk (in words).\n",
    "        overlap (int): The number of overlapping words between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of text chunks.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) <= chunk_size:\n",
    "        return [text]  # Base case: return the text if it fits in one chunk\n",
    "\n",
    "    # Create the current chunk and recurse on the remaining text\n",
    "    current_chunk = \" \".join(words[:chunk_size])\n",
    "    next_text = \" \".join(words[chunk_size - overlap:])  # Prepare the remaining text with overlap\n",
    "    return [current_chunk] + recursive_text_splitter(next_text, chunk_size, overlap)\n",
    "\n",
    "\n",
    "def split_array_into_chunks(array, chunk_size=30, overlap=9):\n",
    "    \"\"\"\n",
    "    Splits each element of an array into chunks using the recursive text splitter.\n",
    "\n",
    "    Args:\n",
    "        array (list): The array of strings to process.\n",
    "        chunk_size (int): The size of each chunk (in words).\n",
    "        overlap (int): The number of overlapping words between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of all the chunks from all elements of the array.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for text in array:\n",
    "        chunks.extend(recursive_text_splitter(text, chunk_size, overlap))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_openai_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    \"\"\"\n",
    "    Fetches the embedding for a given text using OpenAI's embedding model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        model (str): The OpenAI embedding model to use.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The embedding vector for the input text.\n",
    "    \"\"\"\n",
    "    response = openai.Embedding.create(input=text, model=model)\n",
    "    return np.array(response['data'][0]['embedding'])\n",
    "\n",
    "def filter_chunks_by_similarity_openai(chunks, query, benchmark=0.8, model=\"text-embedding-ada-002\"):\n",
    "    \"\"\"\n",
    "    Filters text chunks by calculating cosine similarity against a query using OpenAI embeddings.\n",
    "\n",
    "    Args:\n",
    "        chunks (list): List of text chunks.\n",
    "        query (str): The query string.\n",
    "        benchmark (float): The similarity threshold to filter chunks.\n",
    "        model (str): The OpenAI embedding model to use.\n",
    "\n",
    "    Returns:\n",
    "        list: Chunks that meet the similarity benchmark.\n",
    "    \"\"\"\n",
    "    # Compute the embedding for the query\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "\n",
    "    # Compute embeddings for all chunks\n",
    "    chunk_embeddings = [embedding_model.embed_query(chunk) for chunk in chunks]\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    cosine_similarities = cosine_similarity([query_embedding], chunk_embeddings).flatten()\n",
    "\n",
    "    # Filter chunks that meet the similarity benchmark\n",
    "    filtered_chunks = [chunks[i] for i, score in enumerate(cosine_similarities) if score >= benchmark]\n",
    "\n",
    "    return filtered_chunks\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sample_text = [\n",
    "        \"OpenAI provides state-of-the-art language models. \"\n",
    "        \"You can use embeddings to compare text similarities.\"\n",
    "        \"To highlight the reasoning improvement over GPT-4o, we tested our models on a diverse set of human exams and ML benchmarks. We show that o1 significantly outperforms GPT-4o on the vast majority of these reasoning-heavy tasks. Unless otherwise specified, we evaluated o1 on the maximal test-time compute setting.\",\n",
    "        \"In many reasoning-heavy benchmarks, o1 rivals the performance of human experts. Recent frontier models1 do so well on MATH2 and GSM8K that these benchmarks are no longer effective at differentiating models. We evaluated math performance on AIME, an exam designed to challenge the brightest high school math students in America. On the 2024 AIME exams, GPT-4o only solved on average 12% (1.8/15) of problems. o1 averaged 74% (11.1/15) with a single sample per problem, 83% (12.5/15) with consensus among 64 samples, and 93% (13.9/15) when re-ranking 1000 samples with a learned scoring function. A score of 13.9 places it among the top 500 students nationally and above the cutoff for the USA Mathematical Olympiad.\"\n",
    "    ]\n",
    "    query_text = \"How was the performance of GPT on AIME exams?\"\n",
    "\n",
    "    # Split the sample text into chunks\n",
    "    chunks = split_array_into_chunks(sample_text)\n",
    "    \n",
    "    print(chunks)\n",
    "\n",
    "    # Filter the chunks by similarity to the query\n",
    "    filtered = filter_chunks_by_similarity_openai(chunks, query_text)\n",
    "\n",
    "    print(\"Filtered chunks:\", filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sentence-transformers\n",
      "Version: 3.3.1\n",
      "Summary: State-of-the-Art Text Embeddings\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Nils Reimers <info@nils-reimers.de>, Tom Aarsen <tom.aarsen@huggingface.co>\n",
      "License: Apache 2.0\n",
      "Location: C:\\Users\\Hp\\Desktop\\rs-isf\\ra-isf\\ra-isf\\Lib\\site-packages\n",
      "Requires: huggingface-hub, Pillow, scikit-learn, scipy, torch, tqdm, transformers\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\rs-isf\\ra-isf\\ra-isf\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ra-isf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
